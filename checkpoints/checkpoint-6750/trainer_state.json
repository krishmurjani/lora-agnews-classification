{
  "best_global_step": 6750,
  "best_metric": 0.30582156777381897,
  "best_model_checkpoint": "./results_lora_r16/checkpoint-6750",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 6750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014814814814814815,
      "grad_norm": 2.3345601558685303,
      "learning_rate": 7.333333333333334e-07,
      "loss": 1.4023,
      "step": 100
    },
    {
      "epoch": 0.02962962962962963,
      "grad_norm": 3.689131259918213,
      "learning_rate": 1.474074074074074e-06,
      "loss": 1.3953,
      "step": 200
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 2.701310873031616,
      "learning_rate": 2.214814814814815e-06,
      "loss": 1.3877,
      "step": 300
    },
    {
      "epoch": 0.05925925925925926,
      "grad_norm": 2.1833362579345703,
      "learning_rate": 2.955555555555556e-06,
      "loss": 1.3909,
      "step": 400
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 1.9747850894927979,
      "learning_rate": 3.6962962962962966e-06,
      "loss": 1.3864,
      "step": 500
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 3.165902853012085,
      "learning_rate": 4.437037037037038e-06,
      "loss": 1.3788,
      "step": 600
    },
    {
      "epoch": 0.1037037037037037,
      "grad_norm": 2.0213191509246826,
      "learning_rate": 5.177777777777779e-06,
      "loss": 1.381,
      "step": 700
    },
    {
      "epoch": 0.11851851851851852,
      "grad_norm": 1.7232481241226196,
      "learning_rate": 5.918518518518519e-06,
      "loss": 1.3797,
      "step": 800
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.93344783782959,
      "learning_rate": 6.6592592592592595e-06,
      "loss": 1.3724,
      "step": 900
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 1.9812439680099487,
      "learning_rate": 7.4e-06,
      "loss": 1.3641,
      "step": 1000
    },
    {
      "epoch": 0.16296296296296298,
      "grad_norm": 2.5822863578796387,
      "learning_rate": 8.140740740740742e-06,
      "loss": 1.3603,
      "step": 1100
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.768273115158081,
      "learning_rate": 8.881481481481482e-06,
      "loss": 1.3487,
      "step": 1200
    },
    {
      "epoch": 0.1925925925925926,
      "grad_norm": 3.7504169940948486,
      "learning_rate": 9.622222222222222e-06,
      "loss": 1.344,
      "step": 1300
    },
    {
      "epoch": 0.2074074074074074,
      "grad_norm": 2.5797080993652344,
      "learning_rate": 1.0362962962962964e-05,
      "loss": 1.3264,
      "step": 1400
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.2258501052856445,
      "learning_rate": 1.1103703703703705e-05,
      "loss": 1.3088,
      "step": 1500
    },
    {
      "epoch": 0.23703703703703705,
      "grad_norm": 1.4685392379760742,
      "learning_rate": 1.1844444444444445e-05,
      "loss": 1.2802,
      "step": 1600
    },
    {
      "epoch": 0.2518518518518518,
      "grad_norm": 2.191481351852417,
      "learning_rate": 1.2585185185185187e-05,
      "loss": 1.2374,
      "step": 1700
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.4736261367797852,
      "learning_rate": 1.3325925925925927e-05,
      "loss": 1.1912,
      "step": 1800
    },
    {
      "epoch": 0.2814814814814815,
      "grad_norm": 1.778208613395691,
      "learning_rate": 1.4066666666666669e-05,
      "loss": 1.1346,
      "step": 1900
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 2.818594455718994,
      "learning_rate": 1.4807407407407409e-05,
      "loss": 1.0702,
      "step": 2000
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 2.432600736618042,
      "learning_rate": 1.554814814814815e-05,
      "loss": 0.9962,
      "step": 2100
    },
    {
      "epoch": 0.32592592592592595,
      "grad_norm": 1.71476411819458,
      "learning_rate": 1.628888888888889e-05,
      "loss": 0.9108,
      "step": 2200
    },
    {
      "epoch": 0.34074074074074073,
      "grad_norm": 1.5801458358764648,
      "learning_rate": 1.7029629629629632e-05,
      "loss": 0.8302,
      "step": 2300
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.9349209070205688,
      "learning_rate": 1.777037037037037e-05,
      "loss": 0.7786,
      "step": 2400
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 2.004091501235962,
      "learning_rate": 1.8511111111111112e-05,
      "loss": 0.6948,
      "step": 2500
    },
    {
      "epoch": 0.3851851851851852,
      "grad_norm": 1.2104976177215576,
      "learning_rate": 1.9251851851851854e-05,
      "loss": 0.6612,
      "step": 2600
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.837047815322876,
      "learning_rate": 1.9992592592592595e-05,
      "loss": 0.6118,
      "step": 2700
    },
    {
      "epoch": 0.4148148148148148,
      "grad_norm": 1.6352038383483887,
      "learning_rate": 1.991851851851852e-05,
      "loss": 0.5679,
      "step": 2800
    },
    {
      "epoch": 0.42962962962962964,
      "grad_norm": 2.4668545722961426,
      "learning_rate": 1.9836213991769547e-05,
      "loss": 0.5285,
      "step": 2900
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.298882007598877,
      "learning_rate": 1.975390946502058e-05,
      "loss": 0.4786,
      "step": 3000
    },
    {
      "epoch": 0.45925925925925926,
      "grad_norm": 0.9714305996894836,
      "learning_rate": 1.9671604938271607e-05,
      "loss": 0.4826,
      "step": 3100
    },
    {
      "epoch": 0.4740740740740741,
      "grad_norm": 1.662156581878662,
      "learning_rate": 1.9589300411522635e-05,
      "loss": 0.4675,
      "step": 3200
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 2.0429091453552246,
      "learning_rate": 1.9506995884773663e-05,
      "loss": 0.4645,
      "step": 3300
    },
    {
      "epoch": 0.5037037037037037,
      "grad_norm": 1.3379582166671753,
      "learning_rate": 1.9424691358024692e-05,
      "loss": 0.4256,
      "step": 3400
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 1.9992386102676392,
      "learning_rate": 1.934238683127572e-05,
      "loss": 0.4197,
      "step": 3500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.825028419494629,
      "learning_rate": 1.9260082304526752e-05,
      "loss": 0.4396,
      "step": 3600
    },
    {
      "epoch": 0.5481481481481482,
      "grad_norm": 1.5049124956130981,
      "learning_rate": 1.917777777777778e-05,
      "loss": 0.4488,
      "step": 3700
    },
    {
      "epoch": 0.562962962962963,
      "grad_norm": 1.362392544746399,
      "learning_rate": 1.909547325102881e-05,
      "loss": 0.3673,
      "step": 3800
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.0654222965240479,
      "learning_rate": 1.9013168724279837e-05,
      "loss": 0.4329,
      "step": 3900
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 0.905754029750824,
      "learning_rate": 1.8930864197530865e-05,
      "loss": 0.4239,
      "step": 4000
    },
    {
      "epoch": 0.6074074074074074,
      "grad_norm": 1.3958194255828857,
      "learning_rate": 1.8848559670781893e-05,
      "loss": 0.3683,
      "step": 4100
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.4514237642288208,
      "learning_rate": 1.8766255144032922e-05,
      "loss": 0.3762,
      "step": 4200
    },
    {
      "epoch": 0.6370370370370371,
      "grad_norm": 1.9932072162628174,
      "learning_rate": 1.8683950617283953e-05,
      "loss": 0.3963,
      "step": 4300
    },
    {
      "epoch": 0.6518518518518519,
      "grad_norm": 3.3741116523742676,
      "learning_rate": 1.8601646090534982e-05,
      "loss": 0.3877,
      "step": 4400
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.3081871271133423,
      "learning_rate": 1.851934156378601e-05,
      "loss": 0.3856,
      "step": 4500
    },
    {
      "epoch": 0.6814814814814815,
      "grad_norm": 3.3309969902038574,
      "learning_rate": 1.843703703703704e-05,
      "loss": 0.3911,
      "step": 4600
    },
    {
      "epoch": 0.6962962962962963,
      "grad_norm": 2.106995105743408,
      "learning_rate": 1.8354732510288067e-05,
      "loss": 0.3979,
      "step": 4700
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.638879656791687,
      "learning_rate": 1.8272427983539095e-05,
      "loss": 0.3774,
      "step": 4800
    },
    {
      "epoch": 0.725925925925926,
      "grad_norm": 0.550649106502533,
      "learning_rate": 1.8190123456790127e-05,
      "loss": 0.3861,
      "step": 4900
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 1.5736380815505981,
      "learning_rate": 1.8107818930041155e-05,
      "loss": 0.3835,
      "step": 5000
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 1.3889007568359375,
      "learning_rate": 1.8025514403292183e-05,
      "loss": 0.3936,
      "step": 5100
    },
    {
      "epoch": 0.7703703703703704,
      "grad_norm": 1.3755372762680054,
      "learning_rate": 1.7943209876543212e-05,
      "loss": 0.3838,
      "step": 5200
    },
    {
      "epoch": 0.7851851851851852,
      "grad_norm": 1.218530535697937,
      "learning_rate": 1.786090534979424e-05,
      "loss": 0.3724,
      "step": 5300
    },
    {
      "epoch": 0.8,
      "grad_norm": 3.4830808639526367,
      "learning_rate": 1.777860082304527e-05,
      "loss": 0.359,
      "step": 5400
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 0.8907956480979919,
      "learning_rate": 1.7696296296296297e-05,
      "loss": 0.3683,
      "step": 5500
    },
    {
      "epoch": 0.8296296296296296,
      "grad_norm": 2.074208974838257,
      "learning_rate": 1.761399176954733e-05,
      "loss": 0.3611,
      "step": 5600
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 1.2198864221572876,
      "learning_rate": 1.7531687242798357e-05,
      "loss": 0.346,
      "step": 5700
    },
    {
      "epoch": 0.8592592592592593,
      "grad_norm": 1.1023447513580322,
      "learning_rate": 1.7449382716049382e-05,
      "loss": 0.353,
      "step": 5800
    },
    {
      "epoch": 0.8740740740740741,
      "grad_norm": 2.1502797603607178,
      "learning_rate": 1.7367078189300413e-05,
      "loss": 0.341,
      "step": 5900
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 3.9571163654327393,
      "learning_rate": 1.7284773662551442e-05,
      "loss": 0.3737,
      "step": 6000
    },
    {
      "epoch": 0.9037037037037037,
      "grad_norm": 1.4765806198120117,
      "learning_rate": 1.720246913580247e-05,
      "loss": 0.3604,
      "step": 6100
    },
    {
      "epoch": 0.9185185185185185,
      "grad_norm": 1.3528236150741577,
      "learning_rate": 1.71201646090535e-05,
      "loss": 0.3595,
      "step": 6200
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.0086345672607422,
      "learning_rate": 1.7037860082304527e-05,
      "loss": 0.3533,
      "step": 6300
    },
    {
      "epoch": 0.9481481481481482,
      "grad_norm": 1.5019294023513794,
      "learning_rate": 1.6955555555555555e-05,
      "loss": 0.3758,
      "step": 6400
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 2.285604953765869,
      "learning_rate": 1.6873251028806587e-05,
      "loss": 0.3667,
      "step": 6500
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.0750890970230103,
      "learning_rate": 1.6790946502057615e-05,
      "loss": 0.3493,
      "step": 6600
    },
    {
      "epoch": 0.9925925925925926,
      "grad_norm": 1.0176819562911987,
      "learning_rate": 1.6708641975308643e-05,
      "loss": 0.3293,
      "step": 6700
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.901,
      "eval_f1": 0.9008985320241512,
      "eval_loss": 0.30582156777381897,
      "eval_precision": 0.9014973011265549,
      "eval_recall": 0.901,
      "eval_runtime": 194.3422,
      "eval_samples_per_second": 61.747,
      "eval_steps_per_second": 1.93,
      "step": 6750
    }
  ],
  "logging_steps": 100,
  "max_steps": 27000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.868685258752e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
