{
  "best_global_step": 20250,
  "best_metric": 0.26984283328056335,
  "best_model_checkpoint": "./results_lora_r16/checkpoint-20250",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 20250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014814814814814815,
      "grad_norm": 2.3345601558685303,
      "learning_rate": 7.333333333333334e-07,
      "loss": 1.4023,
      "step": 100
    },
    {
      "epoch": 0.02962962962962963,
      "grad_norm": 3.689131259918213,
      "learning_rate": 1.474074074074074e-06,
      "loss": 1.3953,
      "step": 200
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 2.701310873031616,
      "learning_rate": 2.214814814814815e-06,
      "loss": 1.3877,
      "step": 300
    },
    {
      "epoch": 0.05925925925925926,
      "grad_norm": 2.1833362579345703,
      "learning_rate": 2.955555555555556e-06,
      "loss": 1.3909,
      "step": 400
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 1.9747850894927979,
      "learning_rate": 3.6962962962962966e-06,
      "loss": 1.3864,
      "step": 500
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 3.165902853012085,
      "learning_rate": 4.437037037037038e-06,
      "loss": 1.3788,
      "step": 600
    },
    {
      "epoch": 0.1037037037037037,
      "grad_norm": 2.0213191509246826,
      "learning_rate": 5.177777777777779e-06,
      "loss": 1.381,
      "step": 700
    },
    {
      "epoch": 0.11851851851851852,
      "grad_norm": 1.7232481241226196,
      "learning_rate": 5.918518518518519e-06,
      "loss": 1.3797,
      "step": 800
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 2.93344783782959,
      "learning_rate": 6.6592592592592595e-06,
      "loss": 1.3724,
      "step": 900
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 1.9812439680099487,
      "learning_rate": 7.4e-06,
      "loss": 1.3641,
      "step": 1000
    },
    {
      "epoch": 0.16296296296296298,
      "grad_norm": 2.5822863578796387,
      "learning_rate": 8.140740740740742e-06,
      "loss": 1.3603,
      "step": 1100
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.768273115158081,
      "learning_rate": 8.881481481481482e-06,
      "loss": 1.3487,
      "step": 1200
    },
    {
      "epoch": 0.1925925925925926,
      "grad_norm": 3.7504169940948486,
      "learning_rate": 9.622222222222222e-06,
      "loss": 1.344,
      "step": 1300
    },
    {
      "epoch": 0.2074074074074074,
      "grad_norm": 2.5797080993652344,
      "learning_rate": 1.0362962962962964e-05,
      "loss": 1.3264,
      "step": 1400
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 2.2258501052856445,
      "learning_rate": 1.1103703703703705e-05,
      "loss": 1.3088,
      "step": 1500
    },
    {
      "epoch": 0.23703703703703705,
      "grad_norm": 1.4685392379760742,
      "learning_rate": 1.1844444444444445e-05,
      "loss": 1.2802,
      "step": 1600
    },
    {
      "epoch": 0.2518518518518518,
      "grad_norm": 2.191481351852417,
      "learning_rate": 1.2585185185185187e-05,
      "loss": 1.2374,
      "step": 1700
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.4736261367797852,
      "learning_rate": 1.3325925925925927e-05,
      "loss": 1.1912,
      "step": 1800
    },
    {
      "epoch": 0.2814814814814815,
      "grad_norm": 1.778208613395691,
      "learning_rate": 1.4066666666666669e-05,
      "loss": 1.1346,
      "step": 1900
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 2.818594455718994,
      "learning_rate": 1.4807407407407409e-05,
      "loss": 1.0702,
      "step": 2000
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 2.432600736618042,
      "learning_rate": 1.554814814814815e-05,
      "loss": 0.9962,
      "step": 2100
    },
    {
      "epoch": 0.32592592592592595,
      "grad_norm": 1.71476411819458,
      "learning_rate": 1.628888888888889e-05,
      "loss": 0.9108,
      "step": 2200
    },
    {
      "epoch": 0.34074074074074073,
      "grad_norm": 1.5801458358764648,
      "learning_rate": 1.7029629629629632e-05,
      "loss": 0.8302,
      "step": 2300
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.9349209070205688,
      "learning_rate": 1.777037037037037e-05,
      "loss": 0.7786,
      "step": 2400
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 2.004091501235962,
      "learning_rate": 1.8511111111111112e-05,
      "loss": 0.6948,
      "step": 2500
    },
    {
      "epoch": 0.3851851851851852,
      "grad_norm": 1.2104976177215576,
      "learning_rate": 1.9251851851851854e-05,
      "loss": 0.6612,
      "step": 2600
    },
    {
      "epoch": 0.4,
      "grad_norm": 3.837047815322876,
      "learning_rate": 1.9992592592592595e-05,
      "loss": 0.6118,
      "step": 2700
    },
    {
      "epoch": 0.4148148148148148,
      "grad_norm": 1.6352038383483887,
      "learning_rate": 1.991851851851852e-05,
      "loss": 0.5679,
      "step": 2800
    },
    {
      "epoch": 0.42962962962962964,
      "grad_norm": 2.4668545722961426,
      "learning_rate": 1.9836213991769547e-05,
      "loss": 0.5285,
      "step": 2900
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.298882007598877,
      "learning_rate": 1.975390946502058e-05,
      "loss": 0.4786,
      "step": 3000
    },
    {
      "epoch": 0.45925925925925926,
      "grad_norm": 0.9714305996894836,
      "learning_rate": 1.9671604938271607e-05,
      "loss": 0.4826,
      "step": 3100
    },
    {
      "epoch": 0.4740740740740741,
      "grad_norm": 1.662156581878662,
      "learning_rate": 1.9589300411522635e-05,
      "loss": 0.4675,
      "step": 3200
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 2.0429091453552246,
      "learning_rate": 1.9506995884773663e-05,
      "loss": 0.4645,
      "step": 3300
    },
    {
      "epoch": 0.5037037037037037,
      "grad_norm": 1.3379582166671753,
      "learning_rate": 1.9424691358024692e-05,
      "loss": 0.4256,
      "step": 3400
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 1.9992386102676392,
      "learning_rate": 1.934238683127572e-05,
      "loss": 0.4197,
      "step": 3500
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 3.825028419494629,
      "learning_rate": 1.9260082304526752e-05,
      "loss": 0.4396,
      "step": 3600
    },
    {
      "epoch": 0.5481481481481482,
      "grad_norm": 1.5049124956130981,
      "learning_rate": 1.917777777777778e-05,
      "loss": 0.4488,
      "step": 3700
    },
    {
      "epoch": 0.562962962962963,
      "grad_norm": 1.362392544746399,
      "learning_rate": 1.909547325102881e-05,
      "loss": 0.3673,
      "step": 3800
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.0654222965240479,
      "learning_rate": 1.9013168724279837e-05,
      "loss": 0.4329,
      "step": 3900
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 0.905754029750824,
      "learning_rate": 1.8930864197530865e-05,
      "loss": 0.4239,
      "step": 4000
    },
    {
      "epoch": 0.6074074074074074,
      "grad_norm": 1.3958194255828857,
      "learning_rate": 1.8848559670781893e-05,
      "loss": 0.3683,
      "step": 4100
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.4514237642288208,
      "learning_rate": 1.8766255144032922e-05,
      "loss": 0.3762,
      "step": 4200
    },
    {
      "epoch": 0.6370370370370371,
      "grad_norm": 1.9932072162628174,
      "learning_rate": 1.8683950617283953e-05,
      "loss": 0.3963,
      "step": 4300
    },
    {
      "epoch": 0.6518518518518519,
      "grad_norm": 3.3741116523742676,
      "learning_rate": 1.8601646090534982e-05,
      "loss": 0.3877,
      "step": 4400
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.3081871271133423,
      "learning_rate": 1.851934156378601e-05,
      "loss": 0.3856,
      "step": 4500
    },
    {
      "epoch": 0.6814814814814815,
      "grad_norm": 3.3309969902038574,
      "learning_rate": 1.843703703703704e-05,
      "loss": 0.3911,
      "step": 4600
    },
    {
      "epoch": 0.6962962962962963,
      "grad_norm": 2.106995105743408,
      "learning_rate": 1.8354732510288067e-05,
      "loss": 0.3979,
      "step": 4700
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.638879656791687,
      "learning_rate": 1.8272427983539095e-05,
      "loss": 0.3774,
      "step": 4800
    },
    {
      "epoch": 0.725925925925926,
      "grad_norm": 0.550649106502533,
      "learning_rate": 1.8190123456790127e-05,
      "loss": 0.3861,
      "step": 4900
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 1.5736380815505981,
      "learning_rate": 1.8107818930041155e-05,
      "loss": 0.3835,
      "step": 5000
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 1.3889007568359375,
      "learning_rate": 1.8025514403292183e-05,
      "loss": 0.3936,
      "step": 5100
    },
    {
      "epoch": 0.7703703703703704,
      "grad_norm": 1.3755372762680054,
      "learning_rate": 1.7943209876543212e-05,
      "loss": 0.3838,
      "step": 5200
    },
    {
      "epoch": 0.7851851851851852,
      "grad_norm": 1.218530535697937,
      "learning_rate": 1.786090534979424e-05,
      "loss": 0.3724,
      "step": 5300
    },
    {
      "epoch": 0.8,
      "grad_norm": 3.4830808639526367,
      "learning_rate": 1.777860082304527e-05,
      "loss": 0.359,
      "step": 5400
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 0.8907956480979919,
      "learning_rate": 1.7696296296296297e-05,
      "loss": 0.3683,
      "step": 5500
    },
    {
      "epoch": 0.8296296296296296,
      "grad_norm": 2.074208974838257,
      "learning_rate": 1.761399176954733e-05,
      "loss": 0.3611,
      "step": 5600
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 1.2198864221572876,
      "learning_rate": 1.7531687242798357e-05,
      "loss": 0.346,
      "step": 5700
    },
    {
      "epoch": 0.8592592592592593,
      "grad_norm": 1.1023447513580322,
      "learning_rate": 1.7449382716049382e-05,
      "loss": 0.353,
      "step": 5800
    },
    {
      "epoch": 0.8740740740740741,
      "grad_norm": 2.1502797603607178,
      "learning_rate": 1.7367078189300413e-05,
      "loss": 0.341,
      "step": 5900
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 3.9571163654327393,
      "learning_rate": 1.7284773662551442e-05,
      "loss": 0.3737,
      "step": 6000
    },
    {
      "epoch": 0.9037037037037037,
      "grad_norm": 1.4765806198120117,
      "learning_rate": 1.720246913580247e-05,
      "loss": 0.3604,
      "step": 6100
    },
    {
      "epoch": 0.9185185185185185,
      "grad_norm": 1.3528236150741577,
      "learning_rate": 1.71201646090535e-05,
      "loss": 0.3595,
      "step": 6200
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.0086345672607422,
      "learning_rate": 1.7037860082304527e-05,
      "loss": 0.3533,
      "step": 6300
    },
    {
      "epoch": 0.9481481481481482,
      "grad_norm": 1.5019294023513794,
      "learning_rate": 1.6955555555555555e-05,
      "loss": 0.3758,
      "step": 6400
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 2.285604953765869,
      "learning_rate": 1.6873251028806587e-05,
      "loss": 0.3667,
      "step": 6500
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.0750890970230103,
      "learning_rate": 1.6790946502057615e-05,
      "loss": 0.3493,
      "step": 6600
    },
    {
      "epoch": 0.9925925925925926,
      "grad_norm": 1.0176819562911987,
      "learning_rate": 1.6708641975308643e-05,
      "loss": 0.3293,
      "step": 6700
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.901,
      "eval_f1": 0.9008985320241512,
      "eval_loss": 0.30582156777381897,
      "eval_precision": 0.9014973011265549,
      "eval_recall": 0.901,
      "eval_runtime": 194.3422,
      "eval_samples_per_second": 61.747,
      "eval_steps_per_second": 1.93,
      "step": 6750
    },
    {
      "epoch": 1.0074074074074073,
      "grad_norm": 2.6954739093780518,
      "learning_rate": 1.6626337448559672e-05,
      "loss": 0.3613,
      "step": 6800
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 1.9979091882705688,
      "learning_rate": 1.65440329218107e-05,
      "loss": 0.3455,
      "step": 6900
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 1.047532320022583,
      "learning_rate": 1.646172839506173e-05,
      "loss": 0.3187,
      "step": 7000
    },
    {
      "epoch": 1.0518518518518518,
      "grad_norm": 3.2961668968200684,
      "learning_rate": 1.6379423868312757e-05,
      "loss": 0.3426,
      "step": 7100
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.0590511560440063,
      "learning_rate": 1.629711934156379e-05,
      "loss": 0.3593,
      "step": 7200
    },
    {
      "epoch": 1.0814814814814815,
      "grad_norm": 0.5186784267425537,
      "learning_rate": 1.6214814814814817e-05,
      "loss": 0.3427,
      "step": 7300
    },
    {
      "epoch": 1.0962962962962963,
      "grad_norm": 4.6445112228393555,
      "learning_rate": 1.6132510288065845e-05,
      "loss": 0.3606,
      "step": 7400
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.7589210271835327,
      "learning_rate": 1.6050205761316873e-05,
      "loss": 0.3467,
      "step": 7500
    },
    {
      "epoch": 1.125925925925926,
      "grad_norm": 1.9199941158294678,
      "learning_rate": 1.5967901234567902e-05,
      "loss": 0.3068,
      "step": 7600
    },
    {
      "epoch": 1.1407407407407408,
      "grad_norm": 2.603048801422119,
      "learning_rate": 1.588559670781893e-05,
      "loss": 0.3897,
      "step": 7700
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 1.6463955640792847,
      "learning_rate": 1.5803292181069962e-05,
      "loss": 0.2953,
      "step": 7800
    },
    {
      "epoch": 1.1703703703703703,
      "grad_norm": 2.6916465759277344,
      "learning_rate": 1.572098765432099e-05,
      "loss": 0.3638,
      "step": 7900
    },
    {
      "epoch": 1.1851851851851851,
      "grad_norm": 1.7557463645935059,
      "learning_rate": 1.563868312757202e-05,
      "loss": 0.2874,
      "step": 8000
    },
    {
      "epoch": 1.2,
      "grad_norm": 4.384291172027588,
      "learning_rate": 1.5556378600823047e-05,
      "loss": 0.317,
      "step": 8100
    },
    {
      "epoch": 1.2148148148148148,
      "grad_norm": 1.3913384675979614,
      "learning_rate": 1.5474074074074075e-05,
      "loss": 0.3651,
      "step": 8200
    },
    {
      "epoch": 1.2296296296296296,
      "grad_norm": 2.858231544494629,
      "learning_rate": 1.5391769547325103e-05,
      "loss": 0.3342,
      "step": 8300
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 1.0666340589523315,
      "learning_rate": 1.5309465020576132e-05,
      "loss": 0.3435,
      "step": 8400
    },
    {
      "epoch": 1.2592592592592593,
      "grad_norm": 2.9489285945892334,
      "learning_rate": 1.5227160493827162e-05,
      "loss": 0.3704,
      "step": 8500
    },
    {
      "epoch": 1.2740740740740741,
      "grad_norm": 2.3495185375213623,
      "learning_rate": 1.5144855967078192e-05,
      "loss": 0.3365,
      "step": 8600
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 2.7237954139709473,
      "learning_rate": 1.5062551440329218e-05,
      "loss": 0.3105,
      "step": 8700
    },
    {
      "epoch": 1.3037037037037038,
      "grad_norm": 0.8699588179588318,
      "learning_rate": 1.4980246913580247e-05,
      "loss": 0.3309,
      "step": 8800
    },
    {
      "epoch": 1.3185185185185184,
      "grad_norm": 0.7597241997718811,
      "learning_rate": 1.4897942386831277e-05,
      "loss": 0.3254,
      "step": 8900
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.7787177562713623,
      "learning_rate": 1.4815637860082305e-05,
      "loss": 0.3222,
      "step": 9000
    },
    {
      "epoch": 1.348148148148148,
      "grad_norm": 2.486943483352661,
      "learning_rate": 1.4733333333333335e-05,
      "loss": 0.3392,
      "step": 9100
    },
    {
      "epoch": 1.362962962962963,
      "grad_norm": 0.9802000522613525,
      "learning_rate": 1.4651028806584363e-05,
      "loss": 0.3568,
      "step": 9200
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 1.1995621919631958,
      "learning_rate": 1.4568724279835392e-05,
      "loss": 0.31,
      "step": 9300
    },
    {
      "epoch": 1.3925925925925926,
      "grad_norm": 1.3383139371871948,
      "learning_rate": 1.448641975308642e-05,
      "loss": 0.3865,
      "step": 9400
    },
    {
      "epoch": 1.4074074074074074,
      "grad_norm": 0.5779923796653748,
      "learning_rate": 1.440411522633745e-05,
      "loss": 0.321,
      "step": 9500
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 2.1698577404022217,
      "learning_rate": 1.4321810699588478e-05,
      "loss": 0.3578,
      "step": 9600
    },
    {
      "epoch": 1.4370370370370371,
      "grad_norm": 1.0175268650054932,
      "learning_rate": 1.4239506172839508e-05,
      "loss": 0.2992,
      "step": 9700
    },
    {
      "epoch": 1.4518518518518517,
      "grad_norm": 0.5187331438064575,
      "learning_rate": 1.4157201646090537e-05,
      "loss": 0.3456,
      "step": 9800
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 4.835811138153076,
      "learning_rate": 1.4074897119341563e-05,
      "loss": 0.2899,
      "step": 9900
    },
    {
      "epoch": 1.4814814814814814,
      "grad_norm": 0.918114960193634,
      "learning_rate": 1.3992592592592593e-05,
      "loss": 0.3007,
      "step": 10000
    },
    {
      "epoch": 1.4962962962962962,
      "grad_norm": 1.594614028930664,
      "learning_rate": 1.3910288065843622e-05,
      "loss": 0.2802,
      "step": 10100
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 1.4430830478668213,
      "learning_rate": 1.3827983539094652e-05,
      "loss": 0.3384,
      "step": 10200
    },
    {
      "epoch": 1.525925925925926,
      "grad_norm": 1.719624638557434,
      "learning_rate": 1.374567901234568e-05,
      "loss": 0.3419,
      "step": 10300
    },
    {
      "epoch": 1.5407407407407407,
      "grad_norm": 1.0759913921356201,
      "learning_rate": 1.366337448559671e-05,
      "loss": 0.3174,
      "step": 10400
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 1.209739089012146,
      "learning_rate": 1.3581069958847737e-05,
      "loss": 0.337,
      "step": 10500
    },
    {
      "epoch": 1.5703703703703704,
      "grad_norm": 1.5601379871368408,
      "learning_rate": 1.3498765432098767e-05,
      "loss": 0.2722,
      "step": 10600
    },
    {
      "epoch": 1.585185185185185,
      "grad_norm": 0.8320892453193665,
      "learning_rate": 1.3416460905349795e-05,
      "loss": 0.3923,
      "step": 10700
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.2738949060440063,
      "learning_rate": 1.3334156378600825e-05,
      "loss": 0.3452,
      "step": 10800
    },
    {
      "epoch": 1.6148148148148147,
      "grad_norm": 0.6073727011680603,
      "learning_rate": 1.3251851851851853e-05,
      "loss": 0.3137,
      "step": 10900
    },
    {
      "epoch": 1.6296296296296298,
      "grad_norm": 1.7439855337142944,
      "learning_rate": 1.3169547325102883e-05,
      "loss": 0.312,
      "step": 11000
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 1.4394898414611816,
      "learning_rate": 1.308724279835391e-05,
      "loss": 0.2705,
      "step": 11100
    },
    {
      "epoch": 1.6592592592592592,
      "grad_norm": 2.7415530681610107,
      "learning_rate": 1.3004938271604938e-05,
      "loss": 0.3254,
      "step": 11200
    },
    {
      "epoch": 1.674074074074074,
      "grad_norm": 1.2505996227264404,
      "learning_rate": 1.2922633744855968e-05,
      "loss": 0.3131,
      "step": 11300
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 2.089791774749756,
      "learning_rate": 1.2840329218106997e-05,
      "loss": 0.3156,
      "step": 11400
    },
    {
      "epoch": 1.7037037037037037,
      "grad_norm": 1.0947242975234985,
      "learning_rate": 1.2758024691358027e-05,
      "loss": 0.3325,
      "step": 11500
    },
    {
      "epoch": 1.7185185185185186,
      "grad_norm": 1.062783122062683,
      "learning_rate": 1.2675720164609055e-05,
      "loss": 0.323,
      "step": 11600
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.36725133657455444,
      "learning_rate": 1.2593415637860082e-05,
      "loss": 0.3267,
      "step": 11700
    },
    {
      "epoch": 1.748148148148148,
      "grad_norm": 3.1633553504943848,
      "learning_rate": 1.2511111111111112e-05,
      "loss": 0.339,
      "step": 11800
    },
    {
      "epoch": 1.762962962962963,
      "grad_norm": 2.0432252883911133,
      "learning_rate": 1.242880658436214e-05,
      "loss": 0.3487,
      "step": 11900
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.2215147018432617,
      "learning_rate": 1.234650205761317e-05,
      "loss": 0.3331,
      "step": 12000
    },
    {
      "epoch": 1.7925925925925927,
      "grad_norm": 1.1630346775054932,
      "learning_rate": 1.2264197530864198e-05,
      "loss": 0.3184,
      "step": 12100
    },
    {
      "epoch": 1.8074074074074074,
      "grad_norm": 2.6601998805999756,
      "learning_rate": 1.2181893004115228e-05,
      "loss": 0.2956,
      "step": 12200
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 3.007044792175293,
      "learning_rate": 1.2099588477366255e-05,
      "loss": 0.328,
      "step": 12300
    },
    {
      "epoch": 1.837037037037037,
      "grad_norm": 1.1454881429672241,
      "learning_rate": 1.2017283950617285e-05,
      "loss": 0.3411,
      "step": 12400
    },
    {
      "epoch": 1.8518518518518519,
      "grad_norm": 1.660733699798584,
      "learning_rate": 1.1934979423868313e-05,
      "loss": 0.3465,
      "step": 12500
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 2.2630155086517334,
      "learning_rate": 1.1852674897119343e-05,
      "loss": 0.2818,
      "step": 12600
    },
    {
      "epoch": 1.8814814814814815,
      "grad_norm": 1.290989637374878,
      "learning_rate": 1.1770370370370372e-05,
      "loss": 0.3261,
      "step": 12700
    },
    {
      "epoch": 1.8962962962962964,
      "grad_norm": 0.882768452167511,
      "learning_rate": 1.1688065843621402e-05,
      "loss": 0.2894,
      "step": 12800
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 2.731252431869507,
      "learning_rate": 1.1605761316872428e-05,
      "loss": 0.2846,
      "step": 12900
    },
    {
      "epoch": 1.925925925925926,
      "grad_norm": 3.6394643783569336,
      "learning_rate": 1.1523456790123457e-05,
      "loss": 0.3453,
      "step": 13000
    },
    {
      "epoch": 1.9407407407407407,
      "grad_norm": 1.642109751701355,
      "learning_rate": 1.1441152263374487e-05,
      "loss": 0.3072,
      "step": 13100
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 1.104024887084961,
      "learning_rate": 1.1358847736625515e-05,
      "loss": 0.2739,
      "step": 13200
    },
    {
      "epoch": 1.9703703703703703,
      "grad_norm": 1.3255831003189087,
      "learning_rate": 1.1276543209876545e-05,
      "loss": 0.3388,
      "step": 13300
    },
    {
      "epoch": 1.9851851851851852,
      "grad_norm": 2.138353109359741,
      "learning_rate": 1.1194238683127573e-05,
      "loss": 0.3461,
      "step": 13400
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.9576219320297241,
      "learning_rate": 1.1111934156378602e-05,
      "loss": 0.2836,
      "step": 13500
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.90575,
      "eval_f1": 0.9056838915607716,
      "eval_loss": 0.28333625197410583,
      "eval_precision": 0.9061962081960572,
      "eval_recall": 0.90575,
      "eval_runtime": 194.7081,
      "eval_samples_per_second": 61.631,
      "eval_steps_per_second": 1.926,
      "step": 13500
    },
    {
      "epoch": 2.0148148148148146,
      "grad_norm": 3.1539194583892822,
      "learning_rate": 1.102962962962963e-05,
      "loss": 0.3087,
      "step": 13600
    },
    {
      "epoch": 2.0296296296296297,
      "grad_norm": 1.593941569328308,
      "learning_rate": 1.094732510288066e-05,
      "loss": 0.3137,
      "step": 13700
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 2.8856112957000732,
      "learning_rate": 1.0865020576131688e-05,
      "loss": 0.2887,
      "step": 13800
    },
    {
      "epoch": 2.0592592592592593,
      "grad_norm": 1.6187328100204468,
      "learning_rate": 1.0782716049382718e-05,
      "loss": 0.3043,
      "step": 13900
    },
    {
      "epoch": 2.074074074074074,
      "grad_norm": 1.6431443691253662,
      "learning_rate": 1.0700411522633747e-05,
      "loss": 0.3237,
      "step": 14000
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 2.0363619327545166,
      "learning_rate": 1.0618106995884773e-05,
      "loss": 0.3237,
      "step": 14100
    },
    {
      "epoch": 2.1037037037037036,
      "grad_norm": 1.3738842010498047,
      "learning_rate": 1.0535802469135803e-05,
      "loss": 0.3166,
      "step": 14200
    },
    {
      "epoch": 2.1185185185185187,
      "grad_norm": 1.1534245014190674,
      "learning_rate": 1.0453497942386832e-05,
      "loss": 0.3015,
      "step": 14300
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 5.312602519989014,
      "learning_rate": 1.0371193415637862e-05,
      "loss": 0.2938,
      "step": 14400
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 1.1036343574523926,
      "learning_rate": 1.028888888888889e-05,
      "loss": 0.3139,
      "step": 14500
    },
    {
      "epoch": 2.162962962962963,
      "grad_norm": 2.8995862007141113,
      "learning_rate": 1.0206584362139917e-05,
      "loss": 0.3196,
      "step": 14600
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 2.1180145740509033,
      "learning_rate": 1.0124279835390947e-05,
      "loss": 0.3003,
      "step": 14700
    },
    {
      "epoch": 2.1925925925925926,
      "grad_norm": 3.8361258506774902,
      "learning_rate": 1.0041975308641975e-05,
      "loss": 0.3371,
      "step": 14800
    },
    {
      "epoch": 2.2074074074074073,
      "grad_norm": 1.2304611206054688,
      "learning_rate": 9.959670781893005e-06,
      "loss": 0.3,
      "step": 14900
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 2.2200798988342285,
      "learning_rate": 9.877366255144033e-06,
      "loss": 0.34,
      "step": 15000
    },
    {
      "epoch": 2.237037037037037,
      "grad_norm": 2.811845064163208,
      "learning_rate": 9.795061728395062e-06,
      "loss": 0.2804,
      "step": 15100
    },
    {
      "epoch": 2.251851851851852,
      "grad_norm": 0.5047751069068909,
      "learning_rate": 9.712757201646092e-06,
      "loss": 0.2984,
      "step": 15200
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.9552150964736938,
      "learning_rate": 9.63045267489712e-06,
      "loss": 0.2576,
      "step": 15300
    },
    {
      "epoch": 2.2814814814814817,
      "grad_norm": 0.6271437406539917,
      "learning_rate": 9.548148148148148e-06,
      "loss": 0.3232,
      "step": 15400
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 5.410525321960449,
      "learning_rate": 9.465843621399178e-06,
      "loss": 0.2849,
      "step": 15500
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 3.557685375213623,
      "learning_rate": 9.383539094650207e-06,
      "loss": 0.2969,
      "step": 15600
    },
    {
      "epoch": 2.325925925925926,
      "grad_norm": 1.2781894207000732,
      "learning_rate": 9.301234567901235e-06,
      "loss": 0.2976,
      "step": 15700
    },
    {
      "epoch": 2.3407407407407406,
      "grad_norm": 1.2263400554656982,
      "learning_rate": 9.218930041152263e-06,
      "loss": 0.2919,
      "step": 15800
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 1.2714728116989136,
      "learning_rate": 9.136625514403293e-06,
      "loss": 0.3306,
      "step": 15900
    },
    {
      "epoch": 2.3703703703703702,
      "grad_norm": 0.9471182823181152,
      "learning_rate": 9.054320987654322e-06,
      "loss": 0.2848,
      "step": 16000
    },
    {
      "epoch": 2.3851851851851853,
      "grad_norm": 1.5713797807693481,
      "learning_rate": 8.97201646090535e-06,
      "loss": 0.3018,
      "step": 16100
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.9533001184463501,
      "learning_rate": 8.88971193415638e-06,
      "loss": 0.31,
      "step": 16200
    },
    {
      "epoch": 2.414814814814815,
      "grad_norm": 2.8661139011383057,
      "learning_rate": 8.807407407407408e-06,
      "loss": 0.299,
      "step": 16300
    },
    {
      "epoch": 2.4296296296296296,
      "grad_norm": 1.9022423028945923,
      "learning_rate": 8.725102880658437e-06,
      "loss": 0.3092,
      "step": 16400
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 3.433659791946411,
      "learning_rate": 8.642798353909467e-06,
      "loss": 0.3313,
      "step": 16500
    },
    {
      "epoch": 2.4592592592592593,
      "grad_norm": 1.0984704494476318,
      "learning_rate": 8.560493827160493e-06,
      "loss": 0.3125,
      "step": 16600
    },
    {
      "epoch": 2.474074074074074,
      "grad_norm": 2.3695833683013916,
      "learning_rate": 8.478189300411523e-06,
      "loss": 0.2933,
      "step": 16700
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 4.757386207580566,
      "learning_rate": 8.395884773662552e-06,
      "loss": 0.3089,
      "step": 16800
    },
    {
      "epoch": 2.5037037037037035,
      "grad_norm": 3.075812578201294,
      "learning_rate": 8.31358024691358e-06,
      "loss": 0.3134,
      "step": 16900
    },
    {
      "epoch": 2.5185185185185186,
      "grad_norm": 2.2730889320373535,
      "learning_rate": 8.23127572016461e-06,
      "loss": 0.3152,
      "step": 17000
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.5721409320831299,
      "learning_rate": 8.148971193415638e-06,
      "loss": 0.2957,
      "step": 17100
    },
    {
      "epoch": 2.5481481481481483,
      "grad_norm": 2.134114980697632,
      "learning_rate": 8.066666666666667e-06,
      "loss": 0.2972,
      "step": 17200
    },
    {
      "epoch": 2.562962962962963,
      "grad_norm": 1.9153151512145996,
      "learning_rate": 7.984362139917697e-06,
      "loss": 0.266,
      "step": 17300
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 1.2534511089324951,
      "learning_rate": 7.902057613168725e-06,
      "loss": 0.3061,
      "step": 17400
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 2.191300630569458,
      "learning_rate": 7.819753086419753e-06,
      "loss": 0.3449,
      "step": 17500
    },
    {
      "epoch": 2.6074074074074076,
      "grad_norm": 1.305274248123169,
      "learning_rate": 7.737448559670783e-06,
      "loss": 0.2655,
      "step": 17600
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 2.4426677227020264,
      "learning_rate": 7.655144032921812e-06,
      "loss": 0.2965,
      "step": 17700
    },
    {
      "epoch": 2.637037037037037,
      "grad_norm": 3.624563455581665,
      "learning_rate": 7.57283950617284e-06,
      "loss": 0.3288,
      "step": 17800
    },
    {
      "epoch": 2.651851851851852,
      "grad_norm": 0.5853862166404724,
      "learning_rate": 7.490534979423869e-06,
      "loss": 0.2697,
      "step": 17900
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.5099364519119263,
      "learning_rate": 7.4082304526748985e-06,
      "loss": 0.2145,
      "step": 18000
    },
    {
      "epoch": 2.6814814814814816,
      "grad_norm": 0.7024874091148376,
      "learning_rate": 7.325925925925926e-06,
      "loss": 0.3147,
      "step": 18100
    },
    {
      "epoch": 2.696296296296296,
      "grad_norm": 1.6869380474090576,
      "learning_rate": 7.243621399176955e-06,
      "loss": 0.3267,
      "step": 18200
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 2.17582106590271,
      "learning_rate": 7.161316872427984e-06,
      "loss": 0.3009,
      "step": 18300
    },
    {
      "epoch": 2.725925925925926,
      "grad_norm": 3.5110063552856445,
      "learning_rate": 7.079012345679013e-06,
      "loss": 0.3139,
      "step": 18400
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 1.0783318281173706,
      "learning_rate": 6.996707818930042e-06,
      "loss": 0.2914,
      "step": 18500
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 1.5788743495941162,
      "learning_rate": 6.91440329218107e-06,
      "loss": 0.2987,
      "step": 18600
    },
    {
      "epoch": 2.7703703703703706,
      "grad_norm": 2.3030331134796143,
      "learning_rate": 6.832098765432099e-06,
      "loss": 0.3303,
      "step": 18700
    },
    {
      "epoch": 2.785185185185185,
      "grad_norm": 2.192991256713867,
      "learning_rate": 6.7497942386831285e-06,
      "loss": 0.2901,
      "step": 18800
    },
    {
      "epoch": 2.8,
      "grad_norm": 2.0584771633148193,
      "learning_rate": 6.667489711934157e-06,
      "loss": 0.3277,
      "step": 18900
    },
    {
      "epoch": 2.814814814814815,
      "grad_norm": 1.928436279296875,
      "learning_rate": 6.585185185185186e-06,
      "loss": 0.3037,
      "step": 19000
    },
    {
      "epoch": 2.8296296296296295,
      "grad_norm": 0.7578558921813965,
      "learning_rate": 6.502880658436215e-06,
      "loss": 0.2753,
      "step": 19100
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 1.8077112436294556,
      "learning_rate": 6.420576131687243e-06,
      "loss": 0.3043,
      "step": 19200
    },
    {
      "epoch": 2.859259259259259,
      "grad_norm": 1.4819620847702026,
      "learning_rate": 6.338271604938272e-06,
      "loss": 0.2773,
      "step": 19300
    },
    {
      "epoch": 2.8740740740740742,
      "grad_norm": 1.0615973472595215,
      "learning_rate": 6.255967078189301e-06,
      "loss": 0.2774,
      "step": 19400
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.5929201245307922,
      "learning_rate": 6.173662551440329e-06,
      "loss": 0.3267,
      "step": 19500
    },
    {
      "epoch": 2.9037037037037035,
      "grad_norm": 2.3921735286712646,
      "learning_rate": 6.0913580246913585e-06,
      "loss": 0.3061,
      "step": 19600
    },
    {
      "epoch": 2.9185185185185185,
      "grad_norm": 1.8298910856246948,
      "learning_rate": 6.009053497942388e-06,
      "loss": 0.3272,
      "step": 19700
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 2.1403207778930664,
      "learning_rate": 5.926748971193416e-06,
      "loss": 0.2895,
      "step": 19800
    },
    {
      "epoch": 2.948148148148148,
      "grad_norm": 1.0669193267822266,
      "learning_rate": 5.844444444444445e-06,
      "loss": 0.2585,
      "step": 19900
    },
    {
      "epoch": 2.962962962962963,
      "grad_norm": 1.1211662292480469,
      "learning_rate": 5.762139917695474e-06,
      "loss": 0.2717,
      "step": 20000
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.547529399394989,
      "learning_rate": 5.679835390946502e-06,
      "loss": 0.3107,
      "step": 20100
    },
    {
      "epoch": 2.9925925925925925,
      "grad_norm": 1.7542788982391357,
      "learning_rate": 5.597530864197531e-06,
      "loss": 0.2741,
      "step": 20200
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.91025,
      "eval_f1": 0.9101465963441595,
      "eval_loss": 0.26984283328056335,
      "eval_precision": 0.9105215981054462,
      "eval_recall": 0.91025,
      "eval_runtime": 194.813,
      "eval_samples_per_second": 61.598,
      "eval_steps_per_second": 1.925,
      "step": 20250
    }
  ],
  "logging_steps": 100,
  "max_steps": 27000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.606055776256e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
